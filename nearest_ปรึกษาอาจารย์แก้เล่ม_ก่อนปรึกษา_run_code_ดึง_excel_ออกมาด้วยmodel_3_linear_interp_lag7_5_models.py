# -*- coding: utf-8 -*-
"""Nearest ‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡πà‡∏° ‡∏Å‡πà‡∏≠‡∏ô‡∏õ‡∏£‡∏∂‡∏Å‡∏©‡∏≤ run code ‡∏î‡∏∂‡∏á excel ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢Model-3-Linear_interp_lag7_5-models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16S9uCAwHbVfJipasJVOk3Vc5rb5QT1D9

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î                                     |
| ------- | ---------------------------------------------- |
| 1       | ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏´‡∏∏‡πâ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß (AAPL, MSFT, TSLA, ‚Ä¶)        |
| 2       | ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î‡∏ï‡∏•‡∏≤‡∏î + Interpolation (3 ‡∏ß‡∏¥‡∏ò‡∏µ)       |
| 3       | ‡∏™‡∏£‡πâ‡∏≤‡∏á features (Lag7, SMA, EMA, BB, RSI, MACD) |
| 4       | Scaling train ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô                         |
| 5       | ‡πÉ‡∏ä‡πâ TimeSeriesSplit ‡∏´‡∏£‡∏∑‡∏≠ Sliding 70/15/15      |
| 6       | Train ‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• (Linear, XGB, LSTM, Prophet)   |
| 7       | ‡πÄ‡∏Å‡πá‡∏ö RMSE, R¬≤ ‡∏•‡∏á DataFrame                     |
| 8       | ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå                         |
"""

!pip install pandas_market_calendars
!pip install yfinance scikit-learn xgboost matplotlib
import yfinance as yf
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pandas_market_calendars as mcal
import matplotlib.pyplot as plt

# ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# ------------------------------
# 0. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î list ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# ------------------------------
tickers = ["AAPL", "AMD", "AVGO", "GOOG", "GOOGL", "META", "MSFT", "NVDA"]

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
start_date = datetime(2019, 1, 1)
end_date   = datetime(2024, 12, 31)

all_data_list = []

# ------------------------------
# 1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# ------------------------------
for ticker in tickers:
    print(f"üì• Downloading {ticker} ...")
    data = yf.download(ticker, start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'))

    if data.empty:
        print(f"‚ùå No data for {ticker}")
        continue

    data.reset_index(inplace=True)

    if isinstance(data.columns, pd.MultiIndex):
        data.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in data.columns.values]

    def clean_columns(cols):
        cleaned = []
        for col in cols:
            if 'Date' in col:
                cleaned.append('Date')
            else:
                cleaned.append(col.split()[0])
        return cleaned

    data.columns = clean_columns(data.columns)
    data['Symbol'] = ticker.upper()

    # Make sure 'Volume' column is included correctly
    wanted_cols = ['Date', 'Open','High','Low','Close', 'Volume', 'Symbol']
    data = data[[col for col in wanted_cols if col in data.columns]]

    all_data_list.append(data)

# Concatenate all dataframes in the list into a single dataframe
flat_df = pd.concat(all_data_list, ignore_index=True)

flat_df

# ----------------------------------------------
# 2. Descriptive Statistics ‡∏Ç‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß (‡∏£‡∏ß‡∏° OHLC ‡πÅ‡∏•‡∏∞ Volume)
# ----------------------------------------------

# 1. ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ 'Symbol' ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
# Open, High, Low, Close, Volume
descriptive_stats = flat_df.groupby('Symbol')[['Open', 'High', 'Low', 'Close', 'Volume']].describe()

# 2. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏î‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢
rounding_rules = {
    'Open': 2,
    'High': 2,
    'Low': 2,
    'Close': 2,
    'Volume': 0 # ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢ (Volume) ‡∏õ‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°
}

# 3. ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏õ‡∏±‡∏î‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
for col, decimals in rounding_rules.items():
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏•‡∏±‡∏Å (‡πÄ‡∏ä‡πà‡∏ô 'Open') ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô MultiIndex level 0 ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if col in descriptive_stats.columns.get_level_values(0):
        # ‡∏õ‡∏±‡∏î‡πÄ‡∏®‡∏©‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (count, mean, std, min, 25%, 50%, 75%, max) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÜ
        descriptive_stats.loc[:, col] = descriptive_stats[col].round(decimals)

print("--- Descriptive Statistics of OHLCV (‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Symbol) ---")
print(descriptive_stats)

import pandas as pd
import pandas_market_calendars as mcal

# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á continuous date range
all_dates = pd.DataFrame({'Date': pd.date_range(flat_df['Date'].min(), flat_df['Date'].max())})

# 2. ‡∏Ç‡∏¢‡∏≤‡∏¢ Symbol list ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô
symbols = flat_df['Symbol'].unique()
expanded_list = []
for sym in symbols:
    temp = all_dates.copy()
    temp['Symbol'] = sym
    expanded_list.append(temp)
all_dates_symbols = pd.concat(expanded_list, ignore_index=True)

# 3. merge ‡∏Å‡∏±‡∏ö flat_df ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô
full_df = all_dates_symbols.merge(flat_df, on=['Date','Symbol'], how='left')

# 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á holiday / weekend
nyse = mcal.get_calendar('NYSE')
schedule = nyse.schedule(start_date=full_df['Date'].min(), end_date=full_df['Date'].max())
business_days = schedule.index.normalize()
business_set = set(business_days)

full_df['holiday'] = (~full_df['Date'].isin(business_set)).astype(int)
full_df['weekend'] = full_df['Date'].dt.dayofweek.isin([5,6]).astype(int)

# 5. pre_holiday / post_holiday
def compute_pre_holiday(date):
    return int((date + pd.Timedelta(days=1)) not in business_set)

def compute_post_holiday(date):
    return int((date - pd.Timedelta(days=1)) not in business_set)

full_df['pre_holiday'] = full_df['Date'].apply(compute_pre_holiday)
full_df['post_holiday'] = full_df['Date'].apply(compute_post_holiday)

# 6. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
print(full_df[['Date','Symbol','Close']].head(30))

# ----------------------------
# 1) ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á Close ‡∏î‡πâ‡∏ß‡∏¢ nearest
# ----------------------------
price_cols = ['Close']
for col in price_cols:
    full_df[col] = full_df.groupby('Symbol')[col].transform(
        lambda x: x.interpolate(method='nearest')
    )

flat_df=full_df.copy()

flat_df

full_df=flat_df

# ----------------------------
# 2) ‡∏™‡∏£‡πâ‡∏≤‡∏á lag 7 ‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Close
# ----------------------------
lag_cols = []
for lag in range(1, 8):  # 1 ‡∏ñ‡∏∂‡∏á 7 ‡∏ß‡∏±‡∏ô
    col_name = f'Close_lag{lag}'
    full_df[col_name] = full_df.groupby('Symbol')['Close'].shift(lag)
    lag_cols.append(col_name)

print("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô NaN ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:")
print(full_df[['Close','Open'] + lag_cols].isna().sum())

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
cols_to_show = ['Date','Symbol','Close'] + lag_cols
print(full_df[cols_to_show].head(15))

# 9. ‡∏™‡∏£‡πâ‡∏≤‡∏á Target: Close ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
full_df['Close_next'] = full_df.groupby('Symbol')['Close'].shift(-1)

# 10. ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Matrix X
lag_cols = [f'Close_lag{i}' for i in range(1,8)]  # Close_lag1 ... Close_lag7

X_cols = lag_cols
X = full_df[X_cols]
# Target
y = full_df['Close_next']

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
print("Feature Matrix X:")
print(X.head(10))
print("\nTarget y:")
print(y.head(10))

# 11. ‡∏•‡∏ö row ‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN ‡πÉ‡∏ô X ‡∏´‡∏£‡∏∑‡∏≠ y
data_ml = pd.concat([X, y], axis=1)  # ‡∏£‡∏ß‡∏° X ‡πÅ‡∏•‡∏∞ y ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
data_ml_clean = data_ml.dropna().reset_index(drop=True)

# ‡πÅ‡∏¢‡∏Å‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô X ‡πÅ‡∏•‡∏∞ y
X_clean = data_ml_clean[X_cols]
y_clean = data_ml_clean['Close_next']

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
print("Feature Matrix X (clean):")
print(X_clean.head(10))
print("\nTarget y (clean):")
print(y_clean.head(10))

from sklearn.preprocessing import StandardScaler

# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler
scaler = StandardScaler()

# 2. fit & transform X_clean
X_scaled = scaler.fit_transform(X_clean)

# 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
import pandas as pd
X_scaled_df = pd.DataFrame(X_scaled, columns=X_cols)
print(X_scaled_df.head(10))

from tqdm import tqdm

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å window ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏£‡∏¥‡∏á
window_details = []

window_train = 0.7
window_test = 0.15

for sym in tqdm(symbols, desc="Exporting sliding windows"):
    df_sym = full_df[flat_df['Symbol'] == sym].sort_values('Date').reset_index(drop=True)

    data_ml = df_sym[['Date'] + X_cols + ['Close_next']].dropna().reset_index(drop=True)

    n = len(data_ml)
    step = int(n * window_test)
    start = 0
    window_id = 1

    while start + int(n * window_train) + step <= n:
        train_idx = range(start, start + int(n * window_train))
        test_idx  = range(start + int(n * window_train), start + int(n * window_train) + step)

        # Train/Test Data ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
        train_df = data_ml.iloc[train_idx].copy()
        train_df['Symbol'] = sym
        train_df['Window'] = window_id
        train_df['Set'] = 'Train'

        test_df = data_ml.iloc[test_idx].copy()
        test_df['Symbol'] = sym
        test_df['Window'] = window_id
        test_df['Set'] = 'Test'

        window_details.append(pd.concat([train_df, test_df], axis=0))

        start += step
        window_id += 1

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•
models = {
    'Linear': LinearRegression(),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=100, random_state=42, verbosity=0),
    'SVR': SVR()
}

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
all_results = []

# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤ assign Model Name ‡πÄ‡∏≠‡∏á
model_name = "Linearinterp_lag7_5 models"

# Loop ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Symbol
symbols = flat_df['Symbol'].unique()
lag_cols = [f'Close_lag{i}' for i in range(1, 8)]
X_cols = lag_cols

for sym in symbols:
    df_sym = flat_df[flat_df['Symbol'] == sym].sort_values('Date').reset_index(drop=True)

    # Feature & Target
    X = df_sym[X_cols]
    y = df_sym['Close_next']

    # ‡∏•‡∏ö NaN
    data_ml = pd.concat([X, y], axis=1).dropna().reset_index(drop=True)
    X_clean = data_ml[X_cols]
    y_clean = data_ml['Close_next']

    n = len(X_clean)
    step = int(n * window_test)
    start = 0

    # Sliding Window
    while start + int(n * window_train) + step <= n:
        train_idx = range(start, start + int(n*window_train))
        test_idx  = range(start + int(n*window_train), start + int(n*window_train) + step)

        X_train = X_clean.iloc[train_idx].values
        y_train = y_clean.iloc[train_idx].values
        X_test  = X_clean.iloc[test_idx].values
        y_test  = y_clean.iloc[test_idx].values

        # Scale Training set
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled  = scaler.transform(X_test)

        # Train & Evaluate ‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        for name, model in models.items():
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)

            all_results.append({
                'Symbol': sym,
                'model': name,
                'start_idx': start,
                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                'mae': mean_absolute_error(y_test, y_pred),
                'r2': r2_score(y_test, y_pred),
                'Model Name': model_name
            })

        start += step

# ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô DataFrame
results_df = pd.DataFrame(all_results)

# ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
window_full_df = pd.concat(window_details, axis=0).reset_index(drop=True)

# ‡∏à‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
cols_order = ['Symbol', 'Window', 'Set', 'Date'] + X_cols + ['Close_next']
window_full_df = window_full_df[cols_order]

# export ‡πÄ‡∏õ‡πá‡∏ô Excel (‡∏ó‡∏∏‡∏Å symbol ‡πÅ‡∏•‡∏∞‡∏ó‡∏∏‡∏Å window)
excel_path = "sliding_window_full_details.xlsx"
window_full_df.to_excel(excel_path, index=False)
print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Sliding Window ‡πÅ‡∏•‡πâ‡∏ß: {excel_path}")

# ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ window (start-end)
summary_records = []
for (sym, window), group in window_full_df.groupby(['Symbol', 'Window']):
    train_group = group[group['Set'] == 'Train']
    test_group = group[group['Set'] == 'Test']
    summary_records.append({
        'Symbol': sym,
        'Window': window,
        'Train_Start': train_group['Date'].iloc[0],
        'Train_End': train_group['Date'].iloc[-1],
        'Test_Start': test_group['Date'].iloc[0],
        'Test_End': test_group['Date'].iloc[-1],
        'Train_Size': len(train_group),
        'Test_Size': len(test_group)
    })

summary_df = pd.DataFrame(summary_records)
summary_df.to_excel("sliding_window_summary.xlsx", index=False)
print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏£‡∏∏‡∏õ‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡πà‡∏•‡∏∞ window ‡πÅ‡∏•‡πâ‡∏ß: sliding_window_summary.xlsx")

summary_df

# Export to Excel
results_df.to_excel("results_output.xlsx", index=False)
print("‚úÖ Exported results to results_output.xlsx")

results_df